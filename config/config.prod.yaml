# Production configuration for ekaya-engine
# Scenario: Localhost ekaya-engine + deployed production auth server
# Use this when: Developer downloads ekaya-engine and authenticates with production
# Copy to config.yaml: cp config/config.prod.yaml config.yaml

# Server configuration
port: "3443"
env: "production"
base_url: "http://localhost:3443"  # WHERE THIS INSTANCE RUNS - localhost self-hosted
region_domain: "localhost"
auth_server_url: "https://auth.ekaya.ai"  # WHERE AUTH SERVER IS - deployed production
cookie_domain: ""  # Auto-derived: localhost â†’ "" (no domain restrictions, works over HTTP)

# Authentication configuration
auth:
  enable_verification: true  # Always enabled in production
  jwks_endpoints: "https://auth.ekaya.ai=https://auth.ekaya.ai/.well-known/jwks.json"

# OAuth configuration
oauth:
  client_id: "ekaya-engine"

# Database configuration
# All database connection details should be provided via environment variables
# PGHOST, PGPORT, PGUSER, PGPASSWORD, PGDATABASE, PGTYPE, PGSSLMODE
database:
  max_connections: 50
  max_idle_conns: 10
  type: "postgres"
  ssl_mode: "require"

# Redis configuration
# Host and port should be provided via REDIS_HOST and REDIS_PORT
redis:
  db: 0
  key_prefix: "project:"

# Datasource configuration
datasource:
  connection_ttl_minutes: 5
  max_connections_per_user: 1

# SDAP configuration
# SDAP auth settings should be provided via environment variables
sdap:
  auth:
    enabled: false  # Disabled for local testing against prod auth
    jwks_url: "https://auth.ekaya.ai/.well-known/jwks.json"
    issuer: "https://auth.ekaya.ai"
  connection_ttl_minutes: 5
  max_connections_per_user: 1

# Optional: Ontology extraction configuration
# Ontology extraction iteration settings
# These control the interactive Q&A loop for ontology extraction
ontology_max_iterations: 3       # Maximum Q&A iterations for ontology extraction (1-20 recommended, default: 1)
ontology_target_confidence: 0.8  # Target confidence threshold 0.0-1.0 (default: 0.8)

# Optional: Tool filters (empty list = all tools enabled)
enabled_tools: []

# Community AI configuration (free models hosted on DGX Spark cluster)
# These are OpenAI-compatible endpoints for the community to use
community_ai:
  llm_base_url: "http://sparkone:30000/v1"
  llm_model: "Qwen3-30B-A3B-NVFP4-self"
  embedding_url: "http://sparkone:30001/v1"
  embedding_model: "Qwen/Qwen3-Embedding-0.6B"

# Embedded AI configuration (licensed models for enterprise deployments)
# Same as community for now, but would be customer-hosted in production
embedded_ai:
  llm_base_url: "http://sparktwo:30000/v1"
  llm_model: "Qwen3-30B-A3B-NVFP4"
  embedding_url: "http://sparktwo:30001/v1"
  embedding_model: "Qwen/Qwen3-Embedding-0.6B"
